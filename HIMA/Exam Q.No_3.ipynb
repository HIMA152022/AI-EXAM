{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzq6o7VWUhLAqUtmzwzgJX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GV8KpQ2JJygf"},"outputs":[],"source":["import pandas as pd\n","import re\n","\n","# Load the Excel file\n","df = pd.read_excel(\"dataset_2_2.xlsx\", header=None, names=[\"raw1\"])"]},{"cell_type":"code","source":["# Drop missing values\n","df.dropna(inplace=True)"],"metadata":{"id":"Ekb6HvrbKIxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split text and label at the last semicolon\n","df[['text1', 'label1']] = df['raw1'].str.rsplit(\";\", n=1, expand=True)\n","\n","# Fill missing labels with empty strings\n","df.loc[:, 'label1'] = df['label1'].fillna('')"],"metadata":{"id":"PGfDx_uNKZc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply cleaning\n","df[\"clean_text1\"] = df[\"text1\"].apply(clean_tweet)\n","df = df.sample(n=25, random_state=42)"],"metadata":{"id":"OuK7qcAVKd9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preview\n","print(df[[\"clean_text1\", \"label1\"]].head())"],"metadata":{"id":"QMA7WA27KhDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","import torch\n","from datasets import Dataset, load_metric"],"metadata":{"id":"DgaAqEvNMuAq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare dataset1\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"QgWDUXIdNA-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode texts\n","def tokenize(batch):\n","    return tokenizer(batch['clean_text1'], padding=True, truncation=True)"],"metadata":{"id":"fMLhBIgzNMqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert DataFrame to Dataset\n","dataset1 = Dataset.from_pandas(df[['clean_text1', 'label1']])"],"metadata":{"id":"v5wCCAKENT-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize\n","dataset1 = dataset1.map(tokenize, batched=True)"],"metadata":{"id":"wJx0xKrFOYxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split train/test\n","train_test = dataset1.train_test_split(test_size=0.2)"],"metadata":{"id":"AcJAbaCwd8JX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels))"],"metadata":{"id":"aQmld18OeGVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training args\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=64,\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n",")"],"metadata":{"id":"gHuImkbVeMsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define metric\n","metric = load_metric('f1')\n","\n","def compute_metrics(p):\n","    preds = p.predictions.argmax(-1)\n","    return metric.compute(predictions=preds, references=p.label_ids, average='macro')"],"metadata":{"id":"tNLKb4IYeWI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_test['train'],\n","    eval_dataset=train_test['test'],\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"],"metadata":{"id":"lMvxn4F3ebfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Predictions\n","preds_output = trainer.predict(train_test['test1'])\n","pred_labels = np.argmax(preds_output.predictions, axis=1)\n","true_labels = preds_output.label_ids\n","\n","# Print metrics\n","print(classification_report(true_labels, pred_labels, target_names=labels))\n","\n","# Confusion matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","cm = confusion_matrix(true_labels, pred_labels)\n","plt.figure(figsize=(10,7))\n","sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"ix6ivm2iepMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save_pretrained(\"./emotion_bert_model\")\n","tokenizer.save_pretrained(\"./emotion_bert_model\")"],"metadata":{"id":"aztd-dVHehMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import streamlit as st\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","import torch.nn.functional as F\n","\n","@st.cache(allow_output_mutation=True)\n","def load_model():\n","    tokenizer = BertTokenizer.from_pretrained(\"./emotion_bert_model\")\n","    model = BertForSequenceClassification.from_pretrained(\"./emotion_bert_model\")\n","    model.eval()\n","    return tokenizer, model\n","\n","tokenizer, model = load_model()\n","\n","st.title(\"Tweet Emotion Detection\")\n","\n","user_input = st.text_area(\"Enter Tweet Text:\")\n","\n","if st.button(\"Predict Emotion\"):\n","    inputs = tokenizer(user_input, return_tensors=\"pt\", truncation=True, padding=True)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        probs = F.softmax(outputs.logits, dim=1)\n","    pred_idx = torch.argmax(probs).item()\n","    pred_prob = probs[0][pred_idx].item()\n","    emotion = list(label2id.keys())[list(label2id.values()).index(pred_idx)]\n","\n","    st.write(f\"**Predicted Emotion:** {emotion}\")\n","    st.write(f\"**Confidence:** {pred_prob:.2f}\")\n","\n","    st.write(\"**Probabilities:**\")\n","    for i, label in enumerate(label2id.keys()):\n","        st.write(f\"{label}: {probs[0][i].item():.2f}\")\n"],"metadata":{"id":"opkEEj_4ez98"},"execution_count":null,"outputs":[]}]}